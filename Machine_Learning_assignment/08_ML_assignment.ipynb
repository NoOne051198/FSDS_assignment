{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b230ef",
   "metadata": {},
   "source": [
    "# Assignment - 08 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5dedf",
   "metadata": {},
   "source": [
    "<font size = 3>__1. What exactly is a feature? Give an example to illustrate your point ?__</font>\n",
    "\n",
    "\n",
    "__Ans:__ In machine learning, a feature refers to an individual measurable property or characteristic of a phenomenon that is being analyzed. Features are the inputs to the machine learning algorithms used to make predictions or classifications. For instance, in a spam email detection model, the features could include the frequency of specific words in the email, the length of the email, the presence of links or attachments, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c0521",
   "metadata": {},
   "source": [
    "<font size = 3>__2. What are the various circumstances in which feature construction is required ?__</font>\n",
    "\n",
    "__Ans:__ Feature construction is required in several circumstances, including:\n",
    "\n",
    "1. When the available features are insufficient to capture the underlying patterns in the data.\n",
    "2. When the available features are noisy or redundant and need to be combined or filtered.\n",
    "3. When domain knowledge suggests new features that might improve model performance.\n",
    "4. When working with unstructured data like text or images and features must be extracted manually.\n",
    "5. When trying to incorporate external data sources into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d4be9",
   "metadata": {},
   "source": [
    "<font size = 3>__3. Describe how nominal variables are encoded ?__</font>\n",
    "\n",
    "__Ans:__ Nominal variables are categorical variables that don't have any order. One common method to encode nominal variables is `one-hot encoding`. In this method, each category is transformed into a binary feature, and only one of these features can have a value of 1 for each data instance. __For example,__ suppose we have a nominal variable \"color\" with categories \"red,\" \"green,\" and \"blue.\" In that case, one-hot encoding would create three new binary features: \"color_red,\" \"color_green,\" and \"color_blue,\" and each data instance would have only one of these features set to 1, depending on its color category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323ad56",
   "metadata": {},
   "source": [
    "<font size = 3>__4. Describe how numeric features are converted to categorical features ?__</font>\n",
    "\n",
    "__Ans:__ Converting numerical features to categorical features involves creating a set of discrete, non-overlapping categories or bins based on the numerical values. This can be done by using different techniques, such as equal-width binning, equal-frequency binning, or k-means clustering. Once the numerical values are assigned to the appropriate category, they can be treated as categorical features in the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf863c",
   "metadata": {},
   "source": [
    "<font size = 3>__5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach ?__</font>\n",
    "\n",
    "\n",
    "__Ans:__ The feature selection wrapper approach involves selecting a subset of features that result in the best model performance. It uses a trial-and-error approach by training models with different subsets of features and selecting the best-performing one. The advantage of this approach is that it can lead to better model performance since it selects the most relevant features. However, it can be computationally expensive and may result in overfitting if the dataset is small. It may also miss important features that were not included in the trial subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d8388",
   "metadata": {},
   "source": [
    "<font size = 3>__6. When is a feature considered irrelevant? What can be said to quantify it ?__</font>\n",
    "\n",
    "\n",
    "__Ans:__ A feature is considered irrelevant when it does not contribute to the prediction task or introduces noise into the data. It can be quantified by measuring its correlation with the target variable or checking its importance score from a machine learning model that utilizes the feature. If the correlation or importance score is low, then the feature is considered irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539e689",
   "metadata": {},
   "source": [
    "<font size = 3>__7. When is a function considered redundant? What criteria are used to identify features that could be redundant ?__</font>\n",
    "\n",
    "\n",
    "__Ans:__ A feature is considered redundant when it provides the same information as another feature or a combination of other features. Redundant features can be identified by calculating the correlation between features, and if two or more features have a high correlation coefficient, then one of them can be considered redundant. Another method is to use principal component analysis (PCA) to reduce the dimensionality of the feature space by identifying and removing redundant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e47c71",
   "metadata": {},
   "source": [
    "<font size = 3>__8. What are the various distance measurements used to determine feature similarity ?__</font>\n",
    "\n",
    "__Ans:__ There are several distance measurements used to determine feature similarity in machine learning, including:\n",
    "\n",
    "1. __Euclidean distance:__ The straight-line distance between two points in a feature space.\n",
    "2. __Manhattan distance:__ The distance between two points in a feature space measured along the axes, also known as city block distance.\n",
    "3. __Minkowski distance:__ A generalization of both Euclidean and Manhattan distances, with a parameter p that controls the level of emphasis given to larger differences in feature values.\n",
    "4. __Cosine distance:__ A measure of the angle between two vectors, used to compare the similarity of their directions regardless of their magnitude.\n",
    "5. __Hamming distance:__ A measure of the number of positions at which two binary feature vectors differ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1ac9e",
   "metadata": {},
   "source": [
    "<font size = 3>__9. State difference between Euclidean and Manhattan distances ?__</font>\n",
    "\n",
    "__Ans:__ Euclidean distance measures the shortest straight-line distance between two points in a two- or multi-dimensional space, while Manhattan distance calculates the distance between two points by summing up the absolute differences of their corresponding coordinates along each axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89097329",
   "metadata": {},
   "source": [
    "<font size = 3>__10. Distinguish between feature transformation and feature selection ?__</font>\n",
    "\n",
    "__Ans:__ __Feature transformation__ involves creating new features by applying mathematical or statistical functions to existing ones, while feature selection involves choosing a subset of the existing features to use in the model. Feature transformation is used to improve the model's performance by modifying the feature space, while __feature selection__ aims to reduce the feature space without changing the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576eb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
