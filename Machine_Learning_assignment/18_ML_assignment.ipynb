{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e859201",
   "metadata": {},
   "source": [
    "# Assignment - 18 Solution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a7666",
   "metadata": {},
   "source": [
    "<font size = 3>__1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point ?__</font>\n",
    "\n",
    "__Ans:__ Supervised learning is a type of machine learning where the algorithm learns from labeled data to predict outcomes for new, unseen data. Examples include image classification, sentiment analysis, and spam filtering.\n",
    "\n",
    "Unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data to identify patterns or structures in the data. Examples include clustering, anomaly detection, and dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d88149",
   "metadata": {},
   "source": [
    "<font size = 3>__2. Mention a few unsupervised learning applications ?__</font>\n",
    "\n",
    "\n",
    "__Ans:__ Some examples of unsupervised learning applications include clustering similar data points together, anomaly detection, dimensionality reduction, and pattern recognition in unlabelled data. These techniques can be used in various domains, such as image processing, natural language processing, recommendation systems, and fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b4c9a",
   "metadata": {},
   "source": [
    "<font size = 3>__3. What are the three main types of clustering methods? Briefly describe the characteristics of each ?__</font>\n",
    "\n",
    "__Ans:__ The three main types of clustering methods are:\n",
    "\n",
    "1. __Hierarchical clustering:__ This method involves building clusters by recursively splitting or merging them based on a similarity metric. It can be agglomerative, where each data point starts in its own cluster and clusters are successively merged, or divisive, where all data points start in one cluster and are successively split.\n",
    "\n",
    "2. __Partitioning clustering:__ This method involves partitioning the data into non-overlapping clusters, where each data point belongs to only one cluster. Examples of partitioning clustering algorithms include k-means and k-medoids.\n",
    "\n",
    "3. __Density-based clustering:__ This method involves identifying dense regions of data points in the feature space and assigning them to clusters. Examples of density-based clustering algorithms include DBSCAN and OPTICS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996d143",
   "metadata": {},
   "source": [
    "<font size = 3>__4. Explain how the k-means algorithm determines the consistency of clustering ?__</font>\n",
    "\n",
    "__Ans:__ The k-means algorithm determines the consistency of clustering by minimizing the sum of squared distances between data points and their assigned cluster centroids. It first randomly selects k centroids, where k is the number of desired clusters. Then, it assigns each data point to its nearest centroid and calculates the centroid of each resulting cluster. The process repeats until the cluster centroids no longer change significantly. The algorithm converges when the within-cluster sum of squared distances is minimized, indicating that the data points within each cluster are as similar as possible to each other and as dissimilar as possible to those in other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5779cf",
   "metadata": {},
   "source": [
    "<font size = 3>__5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms ?__</font>\n",
    "\n",
    "\n",
    "__Ans:__ Both k-means and k-medoids are clustering algorithms that aim to group similar data points into clusters. The key difference between these algorithms lies in the way they choose the center of each cluster. while k-means uses the mean as the center of a cluster, k-medoids uses the most centrally located data point, which can make it more robust to outliers and noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c87aa0",
   "metadata": {},
   "source": [
    "<font size = 3>__6. What is a dendrogram, and how does it work? Explain how to do it ?__</font>\n",
    "\n",
    "\n",
    "__Ans:__ A dendrogram is a diagram that is commonly used to display hierarchical relationships between items. It is typically represented as a tree diagram with branches representing the similarities and differences between items. The distance or similarity measure between the items is calculated and plotted along the y-axis while the items are plotted along the x-axis. Dendrograms are frequently used in cluster analysis to show the relationship between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbc66c1",
   "metadata": {},
   "source": [
    "<font size = 3>__7. What exactly is SSE? What role does it play in the k-means algorithm ?__</font>\n",
    "\n",
    "__Ans:__ SSE stands for Sum of Squared Errors, which is a measure of the quality of clustering in the k-means algorithm. SSE measures the sum of the squared distance between each data point and its assigned centroid. The goal of the k-means algorithm is to minimize the SSE, which means that the data points should be as close as possible to their assigned centroid. By minimizing the SSE, k-means aims to find the best possible clusters that group similar data points together and maximize the distance between different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b0b12",
   "metadata": {},
   "source": [
    "<font size = 3>__8. With a step-by-step algorithm, explain the k-means procedure ?__</font>\n",
    "\n",
    "\n",
    "__Ans:__ Here is a step-by-step algorithm for the k-means clustering procedure:\n",
    "\n",
    "1. Determine the number of clusters, K, that you want to create.\n",
    "2. Randomly select K data points from the dataset as initial centroids.\n",
    "3. Assign each data point to the nearest centroid based on the Euclidean distance between the data point and centroid.\n",
    "4. Recalculate the centroids of each cluster by taking the mean of all the data points assigned to it.\n",
    "5. Repeat steps 3-4 until the centroids no longer move or a maximum number of iterations is reached.\n",
    "6. The resulting clusters are the final output of the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569af0ce",
   "metadata": {},
   "source": [
    "<font size = 3>__9. In the sense of hierarchical clustering, define the terms single link and complete link ?__</font>\n",
    "\n",
    "\n",
    "__Ans:__ In hierarchical clustering, single link (also known as the nearest neighbor) refers to a method of defining the distance between two clusters as the distance between their closest data points. Complete link (also known as the furthest neighbor) refers to a method of defining the distance between two clusters as the distance between their farthest data points. These methods are used to determine the distance between clusters in the process of constructing a dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c4173c",
   "metadata": {},
   "source": [
    "<font size = 3>__10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point ?__</font>\n",
    "\n",
    "__Ans:__ The apriori principle is used to identify frequent item sets, which are sets of items that appear together in a transaction or basket frequently. By focusing on these frequent item sets, we can reduce the number of combinations of items that need to be examined in a market basket analysis. \n",
    "\n",
    "For example, imagine a grocery store is trying to analyze the items that are purchased together frequently. They have data on every transaction made in the store. Without the apriori principle, they would need to consider every possible combination of items that could be purchased together, which would be very time-consuming and computationally expensive. However, by using the apriori principle to identify frequent item sets, they can limit their analysis to only those combinations that are likely to be of interest, thus reducing the measurement overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e36f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
